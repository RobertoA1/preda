"""
M√≥dulo de evaluaci√≥n y explicabilidad para el modelo de predicci√≥n de deserci√≥n acad√©mica.
Contiene funciones para calcular m√©tricas, visualizar matriz de confusi√≥n y curva ROC.
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import (
    accuracy_score,
    precision_score,
    recall_score,
    f1_score,
    roc_auc_score,
    confusion_matrix,
    roc_curve,
    classification_report
)
from sklearn.inspection import permutation_importance


def evaluate_model(model, X_test, y_test):
    """
    Calcula m√©tricas de clasificaci√≥n para el modelo.
    
    Args:
        model: Modelo Keras entrenado
        X_test: Datos de prueba (features)
        y_test: Etiquetas reales de prueba
        
    Returns:
        dict: Diccionario con todas las m√©tricas de clasificaci√≥n
            - accuracy: Exactitud del modelo
            - precision: Precisi√≥n (positivos predichos correctamente)
            - recall: Recall/Sensibilidad (positivos reales detectados)
            - f1_score: Media arm√≥nica de precision y recall
            - roc_auc: √Årea bajo la curva ROC
    """
    # Obtener predicciones de probabilidad
    y_pred_proba = model.predict(X_test, verbose=0)
    
    # Convertir probabilidades a clases (umbral 0.5)
    y_pred = (y_pred_proba >= 0.5).astype(int).flatten()
    
    # Asegurar que y_test sea un array plano
    y_true = np.array(y_test).flatten()
    
    # Calcular m√©tricas
    metrics = {
        'accuracy': accuracy_score(y_true, y_pred),
        'precision': precision_score(y_true, y_pred, zero_division=0),
        'recall': recall_score(y_true, y_pred, zero_division=0),
        'f1_score': f1_score(y_true, y_pred, zero_division=0),
        'roc_auc': roc_auc_score(y_true, y_pred_proba)
    }
    
    # Imprimir resumen
    print("=" * 50)
    print("M√âTRICAS DE EVALUACI√ìN DEL MODELO")
    print("=" * 50)
    print(f"Accuracy:  {metrics['accuracy']:.4f}")
    print(f"Precision: {metrics['precision']:.4f}")
    print(f"Recall:    {metrics['recall']:.4f}")
    print(f"F1-Score:  {metrics['f1_score']:.4f}")
    print(f"ROC-AUC:   {metrics['roc_auc']:.4f}")
    print("=" * 50)
    
    # Mostrar reporte de clasificaci√≥n completo
    print("\nReporte de Clasificaci√≥n Detallado:")
    print("-" * 50)
    target_names = ['No Deserta (0)', 'Deserta (1)']
    print(classification_report(y_true, y_pred, target_names=target_names, zero_division=0))
    
    return metrics


def plot_confusion_matrix(y_true, y_pred, save_path=None):
    """
    Genera una matriz de confusi√≥n visual usando seaborn.
    
    Args:
        y_true: Etiquetas reales
        y_pred: Predicciones del modelo (clases, no probabilidades)
        save_path: Ruta para guardar la imagen (opcional)
        
    Returns:
        numpy.ndarray: Matriz de confusi√≥n
    """
    # Asegurar arrays planos
    y_true = np.array(y_true).flatten()
    y_pred = np.array(y_pred).flatten()
    
    # Calcular matriz de confusi√≥n
    cm = confusion_matrix(y_true, y_pred)
    
    # Crear figura
    plt.figure(figsize=(8, 6))
    
    # Crear heatmap con seaborn
    sns.heatmap(
        cm,
        annot=True,
        fmt='d',
        cmap='Blues',
        xticklabels=['No Deserta (0)', 'Deserta (1)'],
        yticklabels=['No Deserta (0)', 'Deserta (1)'],
        annot_kws={'size': 14}
    )
    
    plt.title('Matriz de Confusi√≥n\nPredicci√≥n de Deserci√≥n Acad√©mica', fontsize=14, fontweight='bold')
    plt.ylabel('Valor Real', fontsize=12)
    plt.xlabel('Valor Predicho', fontsize=12)
    
    # A√±adir texto explicativo
    plt.text(0.5, -0.15, 
             f'TN={cm[0,0]} | FP={cm[0,1]} | FN={cm[1,0]} | TP={cm[1,1]}',
             ha='center', va='center', transform=plt.gca().transAxes,
             fontsize=10, style='italic')
    
    plt.tight_layout()
    
    # Guardar si se especifica ruta
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"Matriz de confusi√≥n guardada en: {save_path}")
    
    plt.show()
    
    return cm


def plot_roc_curve(y_true, y_pred_proba, save_path=None):
    """
    Genera la curva ROC con el √°rea bajo la curva.
    
    Args:
        y_true: Etiquetas reales
        y_pred_proba: Probabilidades predichas por el modelo
        save_path: Ruta para guardar la imagen (opcional)
        
    Returns:
        float: √Årea bajo la curva ROC (AUC)
    """
    # Asegurar arrays planos
    y_true = np.array(y_true).flatten()
    y_pred_proba = np.array(y_pred_proba).flatten()
    
    # Calcular curva ROC
    fpr, tpr, thresholds = roc_curve(y_true, y_pred_proba)
    
    # Calcular AUC
    auc = roc_auc_score(y_true, y_pred_proba)
    
    # Crear figura
    plt.figure(figsize=(8, 6))
    
    # Graficar curva ROC
    plt.plot(fpr, tpr, color='darkorange', lw=2, 
             label=f'Curva ROC (AUC = {auc:.4f})')
    
    # L√≠nea diagonal (clasificador aleatorio)
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--',
             label='Clasificador Aleatorio (AUC = 0.50)')
    
    # Configurar gr√°fico
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('Tasa de Falsos Positivos (FPR)', fontsize=12)
    plt.ylabel('Tasa de Verdaderos Positivos (TPR)', fontsize=12)
    plt.title('Curva ROC - Predicci√≥n de Deserci√≥n Acad√©mica', fontsize=14, fontweight='bold')
    plt.legend(loc='lower right', fontsize=10)
    plt.grid(True, alpha=0.3)
    
    # A√±adir punto √≥ptimo (opcional - punto m√°s cercano a esquina superior izquierda)
    optimal_idx = np.argmax(tpr - fpr)
    optimal_threshold = thresholds[optimal_idx]
    plt.scatter(fpr[optimal_idx], tpr[optimal_idx], marker='o', color='red', s=100,
                label=f'Umbral √≥ptimo = {optimal_threshold:.3f}', zorder=5)
    plt.legend(loc='lower right', fontsize=10)
    
    plt.tight_layout()
    
    # Guardar si se especifica ruta
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"Curva ROC guardada en: {save_path}")
    
    plt.show()
    
    return auc


# =============================================================================
# FUNCIONES DE EXPLICABILIDAD
# =============================================================================

def get_feature_importance(model, X_test, y_test, feature_names, n_repeats=10, random_state=42):
    """
    Calcula la importancia de caracter√≠sticas usando permutation importance.
    Implementaci√≥n manual compatible con modelos Keras.
    
    Args:
        model: Modelo Keras entrenado
        X_test: Datos de prueba (features)
        y_test: Etiquetas reales de prueba
        feature_names: Lista o array con nombres de las caracter√≠sticas
        n_repeats: N√∫mero de repeticiones para el c√°lculo (default: 10)
        random_state: Semilla para reproducibilidad
        
    Returns:
        pandas.DataFrame: DataFrame ordenado con importancia de cada caracter√≠stica
            Columnas: feature, importance_mean, importance_std
            
    Raises:
        ValueError: Si las dimensiones no coinciden
        RuntimeError: Si hay error en el c√°lculo de permutation importance
    """
    try:
        # Validar dimensiones
        if X_test.shape[1] != len(feature_names):
            raise ValueError(
                f"N√∫mero de caracter√≠sticas ({X_test.shape[1]}) no coincide "
                f"con n√∫mero de nombres ({len(feature_names)})"
            )
        
        # Asegurar que y_test sea array plano
        y_true = np.array(y_test).flatten()
        X_data = np.array(X_test).copy()
        
        print("Calculando importancia de caracter√≠sticas...")
        print(f"Esto puede tomar un momento ({n_repeats} repeticiones)...")
        
        # Establecer semilla
        np.random.seed(random_state)
        
        # Calcular score base (accuracy)
        y_pred_base = (model.predict(X_data, verbose=0) >= 0.5).astype(int).flatten()
        base_score = accuracy_score(y_true, y_pred_base)
        
        # Calcular importancia para cada caracter√≠stica
        n_features = X_data.shape[1]
        importances = np.zeros((n_features, n_repeats))
        
        for feat_idx in range(n_features):
            for rep in range(n_repeats):
                # Crear copia y permutar la caracter√≠stica
                X_permuted = X_data.copy()
                X_permuted[:, feat_idx] = np.random.permutation(X_permuted[:, feat_idx])
                
                # Calcular score con caracter√≠stica permutada
                y_pred_perm = (model.predict(X_permuted, verbose=0) >= 0.5).astype(int).flatten()
                perm_score = accuracy_score(y_true, y_pred_perm)
                
                # La importancia es la ca√≠da en el score
                importances[feat_idx, rep] = base_score - perm_score
        
        # Calcular media y desviaci√≥n est√°ndar
        importance_mean = importances.mean(axis=1)
        importance_std = importances.std(axis=1)
        
        # Crear DataFrame con resultados
        importance_df = pd.DataFrame({
            'feature': feature_names,
            'importance_mean': importance_mean,
            'importance_std': importance_std
        })
        
        # Ordenar por importancia descendente
        importance_df = importance_df.sort_values(
            'importance_mean', 
            ascending=False
        ).reset_index(drop=True)
        
        print("\n" + "=" * 50)
        print("IMPORTANCIA DE CARACTER√çSTICAS (Top 10)")
        print("=" * 50)
        print(importance_df.head(10).to_string(index=False))
        print("=" * 50)
        
        return importance_df
        
    except ValueError as e:
        print(f"Error de validaci√≥n: {e}")
        raise
    except Exception as e:
        raise RuntimeError(f"Error calculando importancia de caracter√≠sticas: {e}")


def plot_feature_importance(importance_df, top_n=15, save_path=None):
    """
    Genera un gr√°fico de barras horizontales con la importancia de caracter√≠sticas.
    
    Args:
        importance_df: DataFrame con columnas 'feature', 'importance_mean', 'importance_std'
        top_n: N√∫mero de caracter√≠sticas principales a mostrar (default: 15)
        save_path: Ruta para guardar la imagen (opcional)
        
    Returns:
        None
    """
    # Tomar las top N caracter√≠sticas
    df_plot = importance_df.head(top_n).copy()
    
    # Invertir orden para que la m√°s importante quede arriba
    df_plot = df_plot.iloc[::-1]
    
    # Crear figura con tama√±o adaptable
    fig_height = max(6, top_n * 0.4)
    plt.figure(figsize=(10, fig_height))
    
    # Crear paleta de colores profesional (degradado)
    colors = plt.cm.Blues(np.linspace(0.4, 0.8, len(df_plot)))
    
    # Gr√°fico de barras horizontales
    bars = plt.barh(
        df_plot['feature'], 
        df_plot['importance_mean'],
        xerr=df_plot['importance_std'],
        color=colors,
        edgecolor='navy',
        linewidth=0.5,
        capsize=3,
        error_kw={'elinewidth': 1, 'capthick': 1, 'alpha': 0.7}
    )
    
    # A√±adir valores en las barras
    for bar, val in zip(bars, df_plot['importance_mean']):
        plt.text(
            bar.get_width() + 0.002, 
            bar.get_y() + bar.get_height()/2,
            f'{val:.4f}',
            va='center',
            fontsize=9,
            color='dimgray'
        )
    
    # Configuraci√≥n del gr√°fico
    plt.xlabel('Importancia (Permutation Importance)', fontsize=12)
    plt.ylabel('Caracter√≠sticas', fontsize=12)
    plt.title(
        f'Top {top_n} Caracter√≠sticas m√°s Importantes\nPredicci√≥n de Deserci√≥n Acad√©mica',
        fontsize=14, 
        fontweight='bold'
    )
    
    # Ajustar l√≠mites del eje x para dejar espacio a los valores
    x_max = df_plot['importance_mean'].max() + df_plot['importance_std'].max() + 0.02
    plt.xlim(0, x_max)
    
    # A√±adir l√≠nea vertical en 0
    plt.axvline(x=0, color='gray', linewidth=0.8, linestyle='-')
    
    # Grid suave
    plt.grid(axis='x', alpha=0.3, linestyle='--')
    
    plt.tight_layout()
    
    # Guardar si se especifica ruta
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"Gr√°fico de importancia guardado en: {save_path}")
    
    plt.show()


def explain_prediction(model, student_data, feature_names, threshold=0.5):
    """
    Genera una explicaci√≥n textual de la predicci√≥n para un estudiante.
    
    Args:
        model: Modelo Keras entrenado
        student_data: Array con datos de un estudiante (1D o 2D)
        feature_names: Lista o array con nombres de las caracter√≠sticas
        threshold: Umbral de clasificaci√≥n (default: 0.5)
        
    Returns:
        dict: Diccionario con:
            - prediction: Clase predicha (0 o 1)
            - probability: Probabilidad de deserci√≥n
            - risk_level: Nivel de riesgo (Bajo, Medio, Alto, Muy Alto)
            - explanation: Explicaci√≥n textual completa
            - risk_factors: Lista de factores de riesgo principales
            - protective_factors: Lista de factores protectores
    """
    # Asegurar formato correcto (2D para el modelo)
    student_data = np.array(student_data)
    if student_data.ndim == 1:
        student_data = student_data.reshape(1, -1)
    
    # Obtener predicci√≥n
    probability = float(model.predict(student_data, verbose=0)[0][0])
    prediction = 1 if probability >= threshold else 0
    
    # Determinar nivel de riesgo
    if probability < 0.25:
        risk_level = "Bajo"
        risk_emoji = "üü¢"
    elif probability < 0.50:
        risk_level = "Medio"
        risk_emoji = "üü°"
    elif probability < 0.75:
        risk_level = "Alto"
        risk_emoji = "üü†"
    else:
        risk_level = "Muy Alto"
        risk_emoji = "üî¥"
    
    # Analizar valores de caracter√≠sticas
    # Identificar caracter√≠sticas con valores extremos (z-score aproximado)
    values = student_data.flatten()
    
    # Crear lista de (caracter√≠stica, valor, contribuci√≥n potencial)
    feature_analysis = []
    for name, value in zip(feature_names, values):
        feature_analysis.append({
            'name': name,
            'value': value,
            'abs_value': abs(value)  # Para ordenar por magnitud
        })
    
    # Ordenar por valor absoluto (asumiendo datos normalizados)
    feature_analysis.sort(key=lambda x: x['abs_value'], reverse=True)
    
    # Identificar factores de riesgo y protectores
    # (valores positivos altos pueden ser riesgo, negativos pueden ser protectores)
    risk_factors = []
    protective_factors = []
    
    for feat in feature_analysis[:10]:  # Analizar top 10 por magnitud
        if feat['value'] > 0.5:  # Valor alto positivo
            risk_factors.append(f"{feat['name']}: {feat['value']:.2f}")
        elif feat['value'] < -0.5:  # Valor alto negativo
            protective_factors.append(f"{feat['name']}: {feat['value']:.2f}")
    
    # Limitar a los 5 principales de cada tipo
    risk_factors = risk_factors[:5]
    protective_factors = protective_factors[:5]
    
    # Construir explicaci√≥n textual
    explanation_lines = [
        "=" * 60,
        "AN√ÅLISIS DE PREDICCI√ìN - DESERCI√ìN ACAD√âMICA",
        "=" * 60,
        "",
        f"üìä RESULTADO DE LA PREDICCI√ìN:",
        f"   ‚Ä¢ Probabilidad de deserci√≥n: {probability:.1%}",
        f"   ‚Ä¢ Clasificaci√≥n: {'DESERTOR' if prediction == 1 else 'NO DESERTOR'}",
        f"   ‚Ä¢ Nivel de riesgo: {risk_emoji} {risk_level}",
        ""
    ]
    
    if risk_factors:
        explanation_lines.extend([
            "‚ö†Ô∏è  FACTORES DE RIESGO IDENTIFICADOS:",
            *[f"   ‚Ä¢ {factor}" for factor in risk_factors],
            ""
        ])
    else:
        explanation_lines.extend([
            "‚ö†Ô∏è  FACTORES DE RIESGO: No se identificaron factores significativos",
            ""
        ])
    
    if protective_factors:
        explanation_lines.extend([
            "‚úÖ FACTORES PROTECTORES:",
            *[f"   ‚Ä¢ {factor}" for factor in protective_factors],
            ""
        ])
    else:
        explanation_lines.extend([
            "‚úÖ FACTORES PROTECTORES: No se identificaron factores significativos",
            ""
        ])
    
    # Recomendaci√≥n basada en el riesgo
    if risk_level in ["Alto", "Muy Alto"]:
        recommendation = "Se recomienda intervenci√≥n inmediata y seguimiento cercano."
    elif risk_level == "Medio":
        recommendation = "Se sugiere monitoreo preventivo y apoyo acad√©mico."
    else:
        recommendation = "Continuar con seguimiento regular."
    
    explanation_lines.extend([
        "üí° RECOMENDACI√ìN:",
        f"   {recommendation}",
        "",
        "=" * 60
    ])
    
    explanation = "\n".join(explanation_lines)
    
    # Imprimir explicaci√≥n
    print(explanation)
    
    return {
        'prediction': prediction,
        'probability': probability,
        'risk_level': risk_level,
        'explanation': explanation,
        'risk_factors': risk_factors,
        'protective_factors': protective_factors
    }


def save_evaluation_report(metrics, plots_paths=None, output_path='evaluation_report.txt', 
                           format='txt', additional_info=None):
    """
    Genera un reporte de evaluaci√≥n en formato TXT o PDF.
    
    Args:
        metrics: Diccionario con m√©tricas del modelo
            Esperado: {'accuracy': float, 'precision': float, 'recall': float,
                       'f1_score': float, 'roc_auc': float}
        plots_paths: Diccionario con rutas de gr√°ficos generados (opcional)
            Ejemplo: {'confusion_matrix': 'path/cm.png', 'roc_curve': 'path/roc.png'}
        output_path: Ruta del archivo de salida
        format: Formato del reporte ('txt' o 'pdf')
        additional_info: Diccionario con informaci√≥n adicional (opcional)
            Ejemplo: {'model_path': 'models/model.keras', 'n_samples': 1000}
            
    Returns:
        str: Ruta del archivo generado
        
    Raises:
        ValueError: Si el formato no es v√°lido
        ImportError: Si se solicita PDF y fpdf no est√° instalado
    """
    import os
    from datetime import datetime
    
    if format not in ['txt', 'pdf']:
        raise ValueError(f"Formato '{format}' no v√°lido. Use 'txt' o 'pdf'.")
    
    # Construir contenido del reporte
    timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    
    report_lines = [
        "=" * 70,
        "REPORTE DE EVALUACI√ìN - MODELO DE PREDICCI√ìN DE DESERCI√ìN ACAD√âMICA",
        "=" * 70,
        "",
        f"Fecha de generaci√≥n: {timestamp}",
        ""
    ]
    
    # Informaci√≥n adicional
    if additional_info:
        report_lines.extend([
            "-" * 70,
            "INFORMACI√ìN DEL MODELO",
            "-" * 70
        ])
        for key, value in additional_info.items():
            report_lines.append(f"  {key}: {value}")
        report_lines.append("")
    
    # M√©tricas principales
    report_lines.extend([
        "-" * 70,
        "M√âTRICAS DE EVALUACI√ìN",
        "-" * 70
    ])
    
    metric_labels = {
        'accuracy': 'Accuracy (Exactitud)',
        'precision': 'Precision (Precisi√≥n)',
        'recall': 'Recall (Sensibilidad)',
        'f1_score': 'F1-Score',
        'roc_auc': 'ROC-AUC'
    }
    
    for key, label in metric_labels.items():
        if key in metrics:
            value = metrics[key]
            if key == 'roc_auc':
                report_lines.append(f"  {label}: {value:.4f}")
            else:
                report_lines.append(f"  {label}: {value:.4f} ({value:.2%})")
    
    report_lines.append("")
    
    # Interpretaci√≥n de m√©tricas
    report_lines.extend([
        "-" * 70,
        "INTERPRETACI√ìN DE RESULTADOS",
        "-" * 70
    ])
    
    # Evaluar desempe√±o general
    avg_score = np.mean([metrics.get('accuracy', 0), metrics.get('f1_score', 0), 
                         metrics.get('roc_auc', 0)])
    
    if avg_score >= 0.85:
        performance = "EXCELENTE"
        recommendation = "El modelo muestra un desempe√±o sobresaliente."
    elif avg_score >= 0.75:
        performance = "BUENO"
        recommendation = "El modelo tiene buen desempe√±o, con margen de mejora."
    elif avg_score >= 0.65:
        performance = "ACEPTABLE"
        recommendation = "Se recomienda ajustar hiperpar√°metros o aumentar datos."
    else:
        performance = "INSUFICIENTE"
        recommendation = "Se requiere revisi√≥n del modelo y los datos de entrenamiento."
    
    report_lines.extend([
        f"  Desempe√±o general: {performance}",
        f"  Recomendaci√≥n: {recommendation}",
        ""
    ])
    
    # An√°lisis de Recall (importante para deserci√≥n)
    recall = metrics.get('recall', 0)
    if recall >= 0.80:
        recall_analysis = "El modelo detecta la mayor√≠a de estudiantes en riesgo."
    elif recall >= 0.60:
        recall_analysis = "Algunos estudiantes en riesgo podr√≠an no ser detectados."
    else:
        recall_analysis = "ALERTA: Muchos estudiantes en riesgo no est√°n siendo detectados."
    
    report_lines.extend([
        f"  An√°lisis de Recall: {recall_analysis}",
        ""
    ])
    
    # Gr√°ficos generados
    if plots_paths:
        report_lines.extend([
            "-" * 70,
            "VISUALIZACIONES GENERADAS",
            "-" * 70
        ])
        for name, path in plots_paths.items():
            report_lines.append(f"  - {name}: {path}")
        report_lines.append("")
    
    # Pie de p√°gina
    report_lines.extend([
        "=" * 70,
        "Fin del Reporte",
        "Sistema de Predicci√≥n de Deserci√≥n Acad√©mica",
        "=" * 70
    ])
    
    report_content = "\n".join(report_lines)
    
    # Guardar seg√∫n formato
    if format == 'txt':
        # Asegurar extensi√≥n correcta
        if not output_path.endswith('.txt'):
            output_path = output_path.rsplit('.', 1)[0] + '.txt'
        
        # Crear directorio si no existe
        os.makedirs(os.path.dirname(output_path) if os.path.dirname(output_path) else '.', exist_ok=True)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        print(f"‚úÖ Reporte TXT guardado en: {output_path}")
        
    elif format == 'pdf':
        try:
            from fpdf import FPDF
        except ImportError:
            raise ImportError(
                "Para generar PDF instale fpdf: pip install fpdf2\n"
                "Alternativamente, use format='txt'"
            )
        
        # Asegurar extensi√≥n correcta
        if not output_path.endswith('.pdf'):
            output_path = output_path.rsplit('.', 1)[0] + '.pdf'
        
        # Crear directorio si no existe
        os.makedirs(os.path.dirname(output_path) if os.path.dirname(output_path) else '.', exist_ok=True)
        
        # Crear PDF
        pdf = FPDF()
        pdf.add_page()
        pdf.set_auto_page_break(auto=True, margin=15)
        
        # T√≠tulo
        pdf.set_font('Arial', 'B', 16)
        pdf.cell(0, 10, 'REPORTE DE EVALUACI√ìN', ln=True, align='C')
        pdf.cell(0, 10, 'Modelo de Predicci√≥n de Deserci√≥n Acad√©mica', ln=True, align='C')
        pdf.ln(5)
        
        # Fecha
        pdf.set_font('Arial', '', 10)
        pdf.cell(0, 8, f'Fecha: {timestamp}', ln=True)
        pdf.ln(5)
        
        # M√©tricas
        pdf.set_font('Arial', 'B', 12)
        pdf.cell(0, 8, 'M√âTRICAS DE EVALUACI√ìN', ln=True)
        pdf.set_font('Arial', '', 10)
        
        for key, label in metric_labels.items():
            if key in metrics:
                value = metrics[key]
                pdf.cell(0, 6, f'  {label}: {value:.4f}', ln=True)
        
        pdf.ln(5)
        
        # Interpretaci√≥n
        pdf.set_font('Arial', 'B', 12)
        pdf.cell(0, 8, 'INTERPRETACI√ìN', ln=True)
        pdf.set_font('Arial', '', 10)
        pdf.cell(0, 6, f'  Desempe√±o: {performance}', ln=True)
        pdf.multi_cell(0, 6, f'  {recommendation}')
        pdf.multi_cell(0, 6, f'  Recall: {recall_analysis}')
        
        # Gr√°ficos (si existen y son accesibles)
        if plots_paths:
            pdf.ln(5)
            pdf.set_font('Arial', 'B', 12)
            pdf.cell(0, 8, 'VISUALIZACIONES', ln=True)
            pdf.set_font('Arial', '', 10)
            
            for name, path in plots_paths.items():
                if os.path.exists(path):
                    try:
                        pdf.ln(3)
                        pdf.cell(0, 6, f'{name}:', ln=True)
                        pdf.image(path, x=10, w=190)
                        pdf.ln(5)
                    except Exception as e:
                        pdf.cell(0, 6, f'  - {name}: {path} (no se pudo incluir)', ln=True)
                else:
                    pdf.cell(0, 6, f'  - {name}: {path}', ln=True)
        
        # Guardar PDF
        pdf.output(output_path)
        print(f"‚úÖ Reporte PDF guardado en: {output_path}")
    
    return output_path


# =============================================================================
# FUNCIONES AUXILIARES
# =============================================================================

def run_full_evaluation(model_path, X_test_path, y_test_path, output_dir=None):
    """
    Ejecuta la evaluaci√≥n completa del modelo.
    
    Args:
        model_path: Ruta al modelo .keras
        X_test_path: Ruta a X_test.npy
        y_test_path: Ruta a y_test.npy
        output_dir: Directorio para guardar gr√°ficos (opcional)
        
    Returns:
        dict: M√©tricas de evaluaci√≥n
    """
    import os
    from tensorflow.keras.models import load_model
    
    # Cargar modelo y datos
    print("Cargando modelo y datos...")
    model = load_model(model_path)
    X_test = np.load(X_test_path)
    y_test = np.load(y_test_path)
    
    # Evaluar modelo
    metrics = evaluate_model(model, X_test, y_test)
    
    # Obtener predicciones para gr√°ficos
    y_pred_proba = model.predict(X_test, verbose=0)
    y_pred = (y_pred_proba >= 0.5).astype(int).flatten()
    
    # Configurar rutas de guardado
    cm_path = None
    roc_path = None
    if output_dir:
        os.makedirs(output_dir, exist_ok=True)
        cm_path = os.path.join(output_dir, 'confusion_matrix.png')
        roc_path = os.path.join(output_dir, 'roc_curve.png')
    
    # Generar gr√°ficos
    plot_confusion_matrix(y_test, y_pred, save_path=cm_path)
    plot_roc_curve(y_test, y_pred_proba, save_path=roc_path)
    
    return metrics


if __name__ == "__main__":
    # Ejemplo de uso completo
    import os
    from tensorflow.keras.models import load_model
    
    # Rutas por defecto del proyecto
    BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    MODEL_PATH = os.path.join(BASE_DIR, 'models', 'best_model.keras')
    DATA_DIR = os.path.join(BASE_DIR, 'data', 'processed')
    OUTPUT_DIR = os.path.join(DATA_DIR, 'plots')
    
    # Cargar datos y modelo
    print("Cargando modelo y datos...")
    model = load_model(MODEL_PATH)
    X_test = np.load(os.path.join(DATA_DIR, 'X_test.npy'))
    y_test = np.load(os.path.join(DATA_DIR, 'y_test.npy'))
    feature_names = np.load(os.path.join(DATA_DIR, 'feature_names.npy'), allow_pickle=True)
    
    # 1. Ejecutar evaluaci√≥n de m√©tricas
    print("\n" + "="*60)
    print("PARTE 1: EVALUACI√ìN DEL MODELO")
    print("="*60)
    metrics = evaluate_model(model, X_test, y_test)
    
    # 2. Generar gr√°ficos de evaluaci√≥n
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    y_pred_proba = model.predict(X_test, verbose=0)
    y_pred = (y_pred_proba >= 0.5).astype(int).flatten()
    
    plot_confusion_matrix(y_test, y_pred, 
                          save_path=os.path.join(OUTPUT_DIR, 'confusion_matrix.png'))
    plot_roc_curve(y_test, y_pred_proba,
                   save_path=os.path.join(OUTPUT_DIR, 'roc_curve.png'))
    
    # 3. Calcular importancia de caracter√≠sticas
    print("\n" + "="*60)
    print("PARTE 2: EXPLICABILIDAD - IMPORTANCIA DE CARACTER√çSTICAS")
    print("="*60)
    importance_df = get_feature_importance(model, X_test, y_test, feature_names)
    plot_feature_importance(importance_df, top_n=15,
                            save_path=os.path.join(OUTPUT_DIR, 'feature_importance.png'))
    
    # 4. Explicar predicci√≥n de un estudiante aleatorio
    print("\n" + "="*60)
    print("PARTE 3: EXPLICABILIDAD - PREDICCI√ìN INDIVIDUAL")
    print("="*60)
    random_idx = np.random.randint(0, len(X_test))
    student_explanation = explain_prediction(
        model, 
        X_test[random_idx], 
        feature_names,
        threshold=0.5
    )
    
    print("\n‚úÖ Evaluaci√≥n completa finalizada.")
    print(f"üìÅ Gr√°ficos guardados en: {OUTPUT_DIR}")
